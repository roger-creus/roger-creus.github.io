<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
    <link rel="stylesheet" href="main.css">
    <title>RCC</title>
</head>
<body>
    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-menu is-active">
            <div class = "navbar-start">
                <a class="navbar-item">
                    <p class = "subtitle">Roger Creus Castanyer</p>
                </a> 
            </div>
            <div class="navbar-end">
                <a class="navbar-item" href="#aboutme">
                About me
                </a>

                <a class="navbar-item" href="#news">
                News
                </a>

                <a class="navbar-item" href="#publications">
                Publications
                </a>

                <a class="navbar-item" href = "#talks">
                    Talks
                </a>

                <a class="navbar-item" href = "#projects">
                    Projects
                </a>


            </div>
        </div>
    </nav>
    <br>
    <div class = "section">
        <div class = "container" id = "aboutme">
            <div class = "columns is-variable is-8">
                <div class = "column is-one-third">
                    <figure class="image is-150x150" width="640" height="360">
                        <img class="is-rounded" src="./pics/personal.jpg">
                    </figure>
                    <br>
                    <p class = "has-text-centered title is-3">Roger Creus Castanyer</p>
                    <p class = "has-text-centered subtitle is-6">creus99@protonmail.com</p>
                    <p class = "has-text-centered">PhD student @ Mila / UdeM</p>
                    <br>
                    <div class = "levels">
                        <div class = "level">
                            <div class = "level-item">
                                <a href ="https://twitter.com/creus_roger" target="_blank">
                                    <span class="icon">
                                        <i class="fab fa-twitter fa-2x"></i>
                                    </span>
                                </a>
                            </div>
                            <div class = "level-item">
                                <a href ="https://scholar.google.ca/citations?hl=en&user=E3y_txsAAAAJ" target="_blank">
                                    <span class="icon">
                                        <i class="fas fa-user-graduate fa-2x"></i>
                                    </span>
                                </a>
                            </div>
                            <div class = "level-item">
                                <a href ="https://github.com/roger-creus" target="_blank">
                                    <span class="icon">
                                        <i class="fab fa-github fa-2x"></i>
                                    </span>
                                </a>
                            </div>
                            <div class = "level-item">
                                <a href ="./uploads/RogerCreusCV.pdf" target="_blank">
                                    <span class="icon">
                                        <i class="fas fa-file fa-2x"></i>
                                    </span>
                                </a>
                            </div>
                        </div>
                    </div>
                </div>
                <div class = "column">
                    <div class = "content">
                        <p class = "title is-5">
                            About me
                        </p>
                        <p>
                            I am a PhD student at <a href = "https://mila.quebec/en/" target="_blank"> Mila Québec</a> & <a href = "https://diro.umontreal.ca/english/home/" target="_blank">University of Montréal</a> since Fall 2024,
                            working with Professors <a href = "https://psc-g.github.io/" target="_blank">Pablo Samuel Castro</a> and <a href = "https://neo-x.github.io/" target="_blank">Glen Berseth</a>.
                            I obtained my MSc at <a href = "https://mila.quebec/en/" target="_blank">Mila Québec</a> & <a href = "https://diro.umontreal.ca/english/home/" target="_blank">University of Montréal</a> in Montréal, Canada
                            and my BSc in Data Science and Engineering at <a href = "https://dse.upc.edu/en" target="_blank">Universitat Politècnica de Catalunya (UPC)</a> in Barcelona, Spain.
                        </p>
                        <p>
                            <i class="icon fas fa-map-marker-alt"></i> From <strong>Barcelona</strong>, Spain<br>
                            <i class="icon fas fa-map-marker-alt"></i> Currently in <strong>Montreal</strong>, Canada
                        </p>
                        <br>
                        <p class = "title is-5">
                            Research
                        </p>
                        <p>
                            My research aims at developing autonomous agents that can operate in open-ended environments without human supervision. 
                            I am particularly interested in the intersection of Deep Reinforcement Learning (e.g. exploration, representation learning) and Foundation Models (e.g. LLMs, VLMs).
                            I aim to bridge the gap between the high-level reasoning capabilities of Foundation Models and the low-level control capabilities of Reinforcement Learning agents to 
                            enable agents to learn and discover skills in a task-agnostic manner.
                        </p>
                    </div>
                    <div class = "columns">
                        <div class = "column is-centered">
                            <div class = "content">
                                <p class = "title is-5">Interests</p>
                                <ul>
                                    <li>
                                        Deep Reinforcement Learning
                                    </li>
                                    <li>
                                        Unsupervised Reinforcement Learning: World Models and Intrinsic Motivation
                                    </li>
                                    <li>
                                        Foundation Models for Control
                                    </li>
                                </ul>
                            </div>
                        </div>
                        <div class = "column">
                            <div class = "content">
                                <p class = "title is-5">Experience</p>
                                <ul>
                                    <li>
                                        Research Intern @ Ubisoft LaForge (Montreal, Canada)
                                    </li>
                                    <li>
                                        Junior Data Scientist @ HP Inc (Barcelona, Spain)
                                    </li>
                                    <li>
                                        Teaching Assistant @ UPC School (Barcelona, Spain)
                                    </li>
                                    <li>
                                        Research Intern @ UPC (Barcelona, Spain)
                                    </li>
                                    <li>
                                        Basketball Coach @ Sagrada Familia Claror (Barcelona, Spain)
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="section">
        <p id="news" class="title has-text-centered is-4">Recent News</p>
        <div class="container">
            <div class="content">
                <ul class="news-list">
                    <li class="news-item">
                        <strong>June 2025:</strong> "Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning" released as preprint.
                        <a href="https://arxiv.org/pdf/2506.15544" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>February 2025:</strong> Obtained the AI Research Scholarship from Université de Montréal.
                    </li>
                    <li class="news-item">
                        <strong>December 2024:</strong> Obtained the Academic Excellence Scholarship from Université de Montréal.
                    </li>
                    <li class="news-item">
                        <strong>September 2024:</strong> Started my PhD at Université de Montréal and Mila under the supervision of Professors Pablo Samuel Castro and Glen Berseth.
                    </li>
                    <li class="news-item">
                        <strong>August 2024:</strong> Submitted my MSc thesis and obtained my Research MSc from Université de Montréal and Mila.  
                        <a href="https://umontreal.scholaris.ca/items/6102c062-b56d-4065-9005-fd0d64accea1" target="_blank">Read my thesis</a>
                    </li>
                    <li class="news-item">
                        <strong>June 2024:</strong> Received the End-of-Studies Scholarship from Université de Montréal to complete my MSc thesis.
                    </li>
                    <li class="news-item">
                        <strong>May 2024:</strong> Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning published at RLC 2024.  
                        <a href="https://arxiv.org/abs/2405.17243" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>April 2024:</strong> RLeXplore accepted at RL Beyond Rewards workshop @ RLC 2024.  
                        <a href="https://github.com/RLE-Foundation/RLeXplore" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>February 2024:</strong> Improving Intrinsic Exploration by Creating Stationary Objectives published at ICLR 2024.  
                        <a href="https://arxiv.org/abs/2310.18144" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>December 2023:</strong> Improving Intrinsic Exploration by Creating Stationary Objectives accepted at Agent Learning in Open Endedness workshop @ NeurIPS 2023.  
                        <a href="https://arxiv.org/abs/2310.18144" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>December 2023:</strong> Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning accepted at Intrinsically Motivated Open-ended Learning workshop @ NeurIPS 2023.  
                        <a href="https://arxiv.org/abs/2405.17243" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>2021:</strong> PixelEDL accepted at Embodied AI workshop @ CVPR 2021.  
                        <a href="https://imatge.upc.edu/web/sites/default/files/pub/cCreus.pdf" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>2021:</strong> Unsupervised Skill-Discovery and Skill-Learning in Minecraft accepted at Unsupervised Reinforcement Learning workshop @ ICML 2021.  
                        <a href="https://arxiv.org/abs/2107.08398" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>2021:</strong> PiCoEDL accepted at Embodied AI workshop @ CVPR 2021.  
                        <a href="https://imatge.upc.edu/web/sites/default/files/pub/cNieto.pdf" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>2021:</strong> Integration of Convolutional Neural Networks in Mobile Applications accepted at Workshop on AI Engineering @ ICSE 2021.  
                        <a href="https://arxiv.org/abs/2103.07286" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>2021:</strong> Which Design Decisions in AI-enabled Mobile Applications Contribute to Greener AI? published at EMSE Journal.  
                        <a href="https://arxiv.org/abs/2109.15284" target="_blank">Read more</a>
                    </li>
                    <li class="news-item">
                        <strong>2021:</strong> Enhancing sequence-to-sequence modelling for RDF triples to natural text accepted at WebNLG workshop 2021.  
                        <a href="https://aclanthology.org/2020.webnlg-1.5.pdf" target="_blank">Read more</a>
                    </li>
                </ul>
            </div>
        </div>
    </div>
    <div class = "section">
        <p id = "publications" class = "title has-text-centered title is-3"> Publications </p>
        <div class = "container">
            <div class = "columns">
                <div class = "column">
                    <!-- This is a publication -->
                    <a href="https://arxiv.org/abs/2310.18144" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\sofe.png" alt="Publication Image 2">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">Improving Intrinsic Exploration by Creating Stationary Objectives</p>
                                    <p class="subtitle is-6"><strong>Roger Creus Castanyer</strong>, Joshua Romoff, Glen Berseth</p>
                                    <p style="font-weight: 800;">Published at ICLR 2024</p>
                                    <p style="font-weight: 800;">Accepted at Agent Learning in Open Endedness workshop @ NeurIPS 2023.</p>
                                    <br>
                                    
                                    <p>We identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. We introduce the <strong>Stationary Objectives For Exploration (SOFE) framework</strong>. Our experiments show that SOFE improves the agents' performance in challenging exploration problems, including sparse-reward tasks, pixel-based observations, 3D navigation, and procedurally generated environments.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://arxiv.org/abs/2405.17243" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\sadapt.png" alt="Publication Image 7">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning</p>
                                    <p class="subtitle is-6">Adriana Hugessen*, <strong>Roger Creus Castanyer*</strong>, Faisal Mohamed*, Glen Berseth</p>
                                    <p style="font-weight: 800;">Published at RLC 2024</p>
                                    <p style="font-weight: 800;">(Oral Presentation) Accepted at Intrinsically Motivated Open-ended Learning workshop @ NeurIPS 2023.</p>
                                    <br>
                                    
                                    <p>Both surprise-minimizing and surprise-maximizing (curiosity) objectives for unsupervised reinforcement learning (RL) have been shown to be effective in different environments, depending on the environment's level of natural entropy. However, neither method can perform well across all entropy regimes. In an effort to find a single surprise-based method that will encourage emergent behaviors in any environment, we propose an agent that can adapt its objective depending on the entropy conditions it faces, by framing the choice as a multi-armed bandit problem. We devise a novel intrinsic feedback signal for the bandit which captures the ability of the agent to control the entropy in its environment. We demonstrate that such agents can learn to control entropy and exhibit emergent behaviors in both high-and low-entropy regimes.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://github.com/RLE-Foundation/RLeXplore" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\rlexplore.png" alt="Publication Image 7">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning</p>
                                    <p class="subtitle is-6">Mingqi Yuan*, <strong>Roger Creus Castanyer*</strong>, Bo Li, Xin Jin, Wenjun Zeng, Glen Berseth</p>
                                    <p style="font-weight: 800;">Accepted at RL Beyond Rewards workshop @ RLC 2024.</p>
                                    <br>
                                    
                                    <p>Extrinsic rewards can effectively guide reinforcement learning (RL) agents in specific tasks. However, extrinsic rewards frequently fall short in complex environments due to the significant human effort needed for their design and annotation. This limitation underscores the necessity for intrinsic rewards, which offer auxiliary and dense signals and can enable agents to learn in an unsupervised manner. Although various intrinsic reward formulations have been proposed, their implementation and optimization details are insufficiently explored and lack standardization, thereby hindering research progress. To address this gap, we introduce RLeXplore, a unified, highly modularized, and plug-and-play framework offering reliable implementations of eight state-of-the-art intrinsic reward algorithms. Furthermore, we conduct an in-depth study that identifies critical implementation details and establishes well-justified standard practices in intrinsically-motivated RL. The source code for RLeXplore is available at https://github.com/RLE-Foundation/RLeXplore.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://imatge.upc.edu/web/sites/default/files/pub/cCreus.pdf" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\pixelEDL.png" alt="Publication Image 1">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">PixelEDL: Unsupervised Skill Discovery and Learning from Pixels</p>
                                    <p class="subtitle is-6"><strong>Roger Creus Castanyer</strong>, Juan José Nieto, Xavier Giró-i-Nieto</p>
                                    <p style="font-weight: 800;">Accepted at Embodied AI workshop @ CVPR 2021.</p>
                                    <br>
                                    
                                    <p>We tackle embodied visual navigation in a task-agnostic set-up by putting the focus on the unsupervised discovery of skills that provide a good coverage of states. Our approach intersects with empowerment: we address the reward-free skill discovery and learning tasks to discover what can be done in an environment and how.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://arxiv.org/abs/2107.08398" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\URL_mine.png" alt="Publication Image 8">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">Unsupervised Skill-Discovery and Skill-Learning in Minecraft</p>
                                    <p class="subtitle is-6">Juan José Nieto, <strong>Roger Creus Castanyer</strong>, Xavier Giró-i-Nieto</p>
                                    <p style="font-weight: 800;">Accepted at Unsupervised Reinforcement Learning workshop @ ICML 2021.</p>
                                    <br>
                                    
                                    <p>Pre-training Reinforcement Learning agents in a task-agnostic manner has shown promising results. However, previous works still struggle in learning and discovering meaningful skills in high-dimensional state-spaces, such as pixel-spaces. We approach the problem by leveraging unsupervised skill discovery and self-supervised learning of state representations.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://imatge.upc.edu/web/sites/default/files/pub/cNieto.pdf" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\picoEDL.png" alt="Publication Image 5">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">PiCoEDL: Discovery and Learning of Minecraft Navigation Goals from Pixels and Coordinates</p>
                                    <p class="subtitle is-6">Juan José Nieto, <strong>Roger Creus Castanyer</strong>, Xavier Giró-i-Nieto</p>
                                    <p style="font-weight: 800;">Accepted at Embodied AI workshop @ CVPR 2021.</p>
                                    <br>
                                    
                                    <p>Defining a reward function in Reinforcement Learning(RL) is not always possible or very costly. For this reason, there is a great interest in training agents in a task-agnostic manner making use of intrinsic motivations and unsupervised techniques. We hypothesize that RL agents will also benefit from unsupervised pre-trainings with no extrinsic rewards, analogously to how humans mostly learn, especially in the early stages of life.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <!-- This is a publication -->
                    <a href="https://arxiv.org/abs/2103.07286" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\cnn_in_mobile.png" alt="Publication Image 4">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">Integration of Convolutional Neural Networks in Mobile Applications</p>
                                    <p class="subtitle is-6"><strong>Roger Creus Castanyer</strong>, Silverio Martínez-Fernández, Xavier Franch</p>
                                    <p style="font-weight: 800;">Accepted at Workshop on AI Engineering @ ICSE 2021.</p>
                                    <br>
                                    
                                    <p>When building Deep Learning (DL) models, data scientists and software engineers manage the trade-off between their accuracy, or any other suitable success criteria, and their complexity. In an environment with high computational power, a common practice is making the models go deeper by designing more sophisticated architectures. However, in the context of mobile devices, which possess less computational power, keeping complexity under control is a must.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <br>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://arxiv.org/abs/2109.15284" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\designDecisions.png" alt="Publication Image 9">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">Which Design Decisions in AI-enabled Mobile Applications Contribute to Greener AI?</p>
                                    <p class="subtitle is-6"><strong>Roger Creus Castanyer</strong>, Silverio Martínez-Fernández, Xavier Franch</p>
                                    <p style="font-weight: 800;">Published at EMSE Journal</p>
                                    <br>
                                    
                                    <p>The construction, evolution and usage of complex artificial intelligence (AI) models demand expensive computational resources. While currently available high-performance computing environments support well this complexity, the deployment of AI models in mobile devices, which is an increasing trend, is challenging. Our objective is to systematically assess the trade-off between accuracy and complexity when deploying complex AI models (e.g. neural networks) to mobile devices, which have an implicit resource limitation.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://aclanthology.org/2020.webnlg-1.5.pdf" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\webnlg.png" alt="Publication Image 6">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">Enhancing sequence-to-sequence modelling for RDF triples to natural text</p>
                                    <p class="subtitle is-6">Oriol Domingo, David Bergés, Roser Cantenys, <strong>Roger Creus Castanyer</strong>, José Adrian Rodríguez Fonollosa</p>
                                    <p style="font-weight: 800;">Accepted at WebNLG workshop 2021</p>
                                    <br>
                                    
                                    <p>This work establishes key guidelines on how, which and when Machine Translation (MT) techniques are worth applying to RDF-to-Text task. Not only do we apply and compare the most prominent MT architecture, the Transformer, but we also analyze state-of-the-art techniques such as Byte Pair Encoding or Back Translation to demonstrate an improvement in generalization.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://arxiv.org/abs/2005.13156" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\datasheets.png" alt="Publication Image 10">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">MT-adapted datasheets for datasets: Template and repository</p>
                                    <p class="subtitle is-6">Marta Costa-jussà, <strong>Roger Creus Castanyer</strong>, Oriol Domingo, Albert Domínguez, Miquel Escobar, Cayetana López, Marina Garcia, Margarita Geleta</p>
                                    <p>In this report we are taking the standardized model proposed by Gebru et al. (2018) for documenting the popular machine translation datasets of the EuroParl (Koehn, 2005) and News-Commentary (Barrault et al., 2019). Within this documentation process, we have adapted the original datasheet to the particular case of data consumers within the Machine Translation area. We are also proposing a repository for collecting the adapted datasheets in this research area.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://upcommons.upc.edu/handle/2117/353772" target="_blank">
                        <div class="box publication">
                            <div class="columns is-vcentered">
                                <div class="column is-one-quarter">
                                    <figure class="image is-256x256">
                                        <img src="pics\pixelEDL.png" alt="Publication Image 3">
                                    </figure>
                                </div>
                                <div class="column">
                                    <p class="title is-5">Unsupervised Skill Learning from Pixels</p>
                                    <p class="subtitle is-6"><strong>Roger Creus Castanyer</strong></p>
                                    <p style="font-weight: 600;">BSc Thesis</p>
                                    <br>
                                    
                                    <p>This work focuses on the self-acquirement of the fundamental task-agnostic knowledge available within an environment. The aim is to discover and learn baseline representations and behaviors that can later be useful for solving embodied visual navigation downstream tasks.</p>
                                </div>
                            </div>
                        </div>  
                    </a>
                    <br>
                </div>
            </div>
        </div>
    </div>
    <div class = "section">
        <p id = "talks" class = "title has-text-centered title is-3"> Talks </p>
        <div class = "container">
            <div class = "columns">
                <div class = "column">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/uclhK0L6mb0?si=SSs7OnxSl4RZF0CR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    <br>
                    <br>
                    <br>
                    <br>
                    <iframe width="500rem" height="315" src="https://www.youtube.com/embed/zv30OKb4nDE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
                <div class = "column">
                    <iframe width="500rem" height="315" src="https://www.youtube.com/embed/9GxBfwnOwDQ?start=1301" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <br>
                    <br>
                    <br>
                    <br>
                    <iframe width="500rem" height="315" src="https://www.youtube.com/embed/bRDTU5YtaSQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
            </div>

        </div>
    </div>
    <div class = "section">
        <p id = "projects" class = "title has-text-centered title is-3"> Projects </p>
        <div class = "container">
            <div class = "columns">
                <div class = "column">
                    <!-- This is a publication -->
                    <a href="https://github.com/roger-creus/centralized-control-lux" target="_blank">
                        <div class = "box publication">
                            <p class = "title is-5"> Centralized control for multi-agent RL in a complex Real-Time-Strategy game </p>
                            <p> This repository contains the source code for the project "Centralized control for multi-agent RL in a complex Real-Time-Strategy game", which was submitted as the final project in the COMP579 - Reinforcement Learning course at McGill given by Prof. Doina Precup in Winter 2023. </p>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://github.com/roger-creus/blokus-ai" target="_blank">
                        <div class = "box publication">
                            <p class = "title is-5"> Blokus RL Learning Environment </p>
                            <p> This project is an implementation of the Blokus board game environment using the Gymnasium framework. This environment is designed to be used for training AI agents to play Blokus. </p>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://github.com/yuyecreus/Ball-Sort-Learning-Environment" target="_blank">
                        <div class = "box publication">
                            <p class = "title is-5"> Ball Sort RL Learning Environment </p>
                            <p> This project provides a Reinforcement Learning Environment for the Ball Sort Color Puzzle Game. The environment is based on the Open AI's Gym framework. It also provides baseline Deep Reinforcement Learning models that solve some levels of the game. </p>
                        </div>  
                    </a>
                </div>
            
                <div class = "column">
                     <!-- This is a publication -->
                     <a href="https://github.com/roger-creus/xgenius" target="_blank">
                        <div class = "box publication">
                            <p class = "title is-5"> xgenius </p>
                            <p>xgenius is a command-line tool for managing remote jobs and containerized experiments across multiple clusters. It simplifies the process of building Docker images, converting them to Singularity format, and submitting jobs to clusters using SLURM.</p>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://github.com/RLE-Foundation/rllte" target="_blank">
                        <div class = "box publication">
                            <p class = "title is-5"> RLLTE </p>
                            <p>RLLTE: Long-Term Evolution Project of Reinforcement Learning</p>
                        </div>  
                    </a>
                    <br>
                    <!-- This is a publication -->
                    <a href="https://github.com/yuyecreus/Wave-Defense-Learning-Environment" target="_blank">
                        <div class = "box publication">
                            <p class = "title is-5"> Wave Defense RL Learning Environment</p>
                            <p>This project provides a Reinforcement Learning Environment for the custom Wave Defense game. The environment is based on the Open AI's Gym framework. It also provides baseline Deep Reinforcement Learning models that solve the game.</p>
                            <p>Details also in this video: https://www.youtube.com/watch?v=VOmj7_nnPJ0&t=1s&ab_channel=RogerCreusCastanyer</p>
                        </div>  
                    </a>
                    <br>
                     <!-- This is a publication -->
                     <a href="https://github.com/yuyecreus/MAE-Neuroevolution" target="_blank">
                        <div class = "box publication">
                            <p class = "title is-5"> Learning to play a simple game with Genetic Neuroevolution </p>
                            <p> This project provides an implementation of a Genetic Neuroevolution algorithm in Matlab that learns to play a custom game. </p>
                        </div>  
                    </a>
                    <br>
                </div>
                <div class = "column">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/bx5QVWGaPG4?si=WiRZE-1QlToeqhct" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    <br>
                    <br>
                    <br>
                    <br>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/PiYwwDsPQOI?si=cliucRNua3ew5I6I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>
            </div>
        </div>
    </div>
</body>

<script>
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
    anchor.addEventListener('click', function (e) {
        e.preventDefault();

        document.querySelector(this.getAttribute('href')).scrollIntoView({
            behavior: 'smooth'
        });
    });
});

</script>



